# 书生·浦语大模型全链路开源体系

## 大模型成为发展人工智能的重要途经

### 专用模型

针对特定任务，一个模型解决一个问题。
- 语音识别
- 图像分类
- 人脸识别

### 通用大模型

一个模型应对多种任务，多种模态

## InternLM2的体系

### 2种规格

- 7B
    - 轻量级研究和应用
    - 轻便但性能不俗
- 20B
    - 复杂的实用场景
    - 综合性能更强

### 3个版本

每种规格包含三个模型版本：

- InternLM2-Base
    - 高质量、可塑性强的模型基座，深度领域适配的起点
- InternLM2
    - 大部分应用的推荐基座，通用语言能力好
- InternLM2-Chat
    - 经过SFT和RLHF优化对话交互，指令遵循，共情聊天，调用工具

## 回归语言建模本质

新一代数据清洗技术：
- 多维度数据价值评估
- 高质量语料驱动的数据富集
    - 基于高质量语料的特征富集互联网类似语料
- 有针对性的数据补齐
    - 加强世界知识、数代码能力


## InternLM2主要亮点

- 超长上下文，支持200K token
- 综合性能提升
- 精准指令跟随、结构化创作
- 提升工具调用能力：
    - 调用搜索、计算、代码解释器完成复杂任务
    - 配合代码解释器能完成积分求解
- 内生数理能力和数据分析能力：
    - 高准确率、复杂运算和求解

## 从模型到应用

模型成为智能客服、个人助手、行业应用
![典型流程](/imgs/典型流程.png)

### 全链条开源体系
1. 数据：
    - 书生万卷：书生万卷1.0、书生万卷CC 
        - 数据获取：OpenDataLab

2. 预训练：InternLM2-Train
3. 微调：Xtuner
    - 覆盖NVIDIA 20系以上显卡
    - 最低8GB显存微调7B模型
    - 增量续训：
        - 学习垂直领域知识
        - 文章、书籍、代码
    - 有监督微调：
        - 理解对话指令、注入少量领域知识
        - 高质量对话、问答数据

    - 全量参数微调
    - 部分参数微调
4. 部署：LMDeploy
    - 轻量化
    - 推理
    - 服务
5. 评测：OpenCompass司南大模型评测体系
6. 应用：Lagent AGentLego

